{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hungry-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "loved-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dims, capacity):\n",
    "        super(Encoder, self).__init__()\n",
    "        c = capacity\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc = nn.Linear(in_features=c*2*56*56, out_features=latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "comprehensive-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims, capacity):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.capacity = capacity\n",
    "        c = capacity\n",
    "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*56*56)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), self.capacity*2, 56, 56)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "conscious-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims, capacity):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dims, capacity)\n",
    "        self.decoder = Decoder(latent_dims, capacity)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        x_recon = self.decoder(latent)\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "stuck-praise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1902, -0.2515, -0.2436,  ..., -0.1444, -0.2528, -0.1960],\n",
       "          [-0.2400, -0.0931, -0.2858,  ..., -0.2936, -0.1930, -0.0973],\n",
       "          [-0.1969, -0.2048, -0.3651,  ..., -0.0318, -0.2925, -0.1465],\n",
       "          ...,\n",
       "          [-0.2126, -0.1665, -0.2605,  ..., -0.0445, -0.1612, -0.1794],\n",
       "          [-0.1907, -0.0835, -0.1628,  ..., -0.1932, -0.3620, -0.0912],\n",
       "          [-0.1907, -0.1681, -0.1871,  ..., -0.2092, -0.1916, -0.1450]]],\n",
       "\n",
       "\n",
       "        [[[-0.1882, -0.2578, -0.2616,  ..., -0.1309, -0.2510, -0.1985],\n",
       "          [-0.2173, -0.1150, -0.2659,  ..., -0.2456, -0.2049, -0.0919],\n",
       "          [-0.1961, -0.1965, -0.3515,  ..., -0.0536, -0.3299, -0.1486],\n",
       "          ...,\n",
       "          [-0.2108, -0.1696, -0.2627,  ..., -0.0566, -0.1207, -0.1768],\n",
       "          [-0.2011, -0.1151, -0.2192,  ..., -0.1859, -0.3358, -0.0850],\n",
       "          [-0.1787, -0.1800, -0.1912,  ..., -0.2132, -0.2159, -0.1405]]],\n",
       "\n",
       "\n",
       "        [[[-0.1966, -0.2435, -0.2434,  ..., -0.1363, -0.2432, -0.1926],\n",
       "          [-0.2564, -0.0641, -0.2780,  ..., -0.2818, -0.1945, -0.0898],\n",
       "          [-0.1958, -0.2216, -0.3344,  ..., -0.0340, -0.2952, -0.1272],\n",
       "          ...,\n",
       "          [-0.2056, -0.1657, -0.2947,  ..., -0.0343, -0.1785, -0.1677],\n",
       "          [-0.2011, -0.0804, -0.1975,  ..., -0.1717, -0.3635, -0.0951],\n",
       "          [-0.1817, -0.1650, -0.1993,  ..., -0.2089, -0.1861, -0.1625]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.1957, -0.2559, -0.2523,  ..., -0.1340, -0.2578, -0.1953],\n",
       "          [-0.2331, -0.0924, -0.2860,  ..., -0.2730, -0.1673, -0.0771],\n",
       "          [-0.2008, -0.2145, -0.3670,  ..., -0.0168, -0.3109, -0.1372],\n",
       "          ...,\n",
       "          [-0.2305, -0.1554, -0.2629,  ..., -0.0275, -0.1486, -0.1712],\n",
       "          [-0.1763, -0.0851, -0.1686,  ..., -0.2039, -0.3701, -0.1042],\n",
       "          [-0.1850, -0.1641, -0.2021,  ..., -0.2008, -0.1879, -0.1487]]],\n",
       "\n",
       "\n",
       "        [[[-0.1925, -0.2535, -0.2418,  ..., -0.1465, -0.2431, -0.1941],\n",
       "          [-0.2440, -0.0969, -0.2863,  ..., -0.3001, -0.2023, -0.1105],\n",
       "          [-0.1935, -0.2156, -0.3541,  ..., -0.0555, -0.2922, -0.1429],\n",
       "          ...,\n",
       "          [-0.2058, -0.1731, -0.2677,  ..., -0.0497, -0.1631, -0.1728],\n",
       "          [-0.1998, -0.0787, -0.1739,  ..., -0.1797, -0.3517, -0.0786],\n",
       "          [-0.1930, -0.1678, -0.1827,  ..., -0.2251, -0.2078, -0.1444]]],\n",
       "\n",
       "\n",
       "        [[[-0.1929, -0.2654, -0.2614,  ..., -0.1364, -0.2509, -0.1878],\n",
       "          [-0.2336, -0.1072, -0.2613,  ..., -0.2555, -0.1845, -0.0813],\n",
       "          [-0.2011, -0.2126, -0.3677,  ..., -0.0250, -0.3231, -0.1418],\n",
       "          ...,\n",
       "          [-0.2157, -0.1809, -0.2693,  ..., -0.0433, -0.1385, -0.1598],\n",
       "          [-0.1897, -0.1082, -0.1976,  ..., -0.1906, -0.3428, -0.0933],\n",
       "          [-0.1844, -0.1789, -0.1897,  ..., -0.2193, -0.1961, -0.1517]]]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoEncoder(latent_dims=10, capacity=64).to(device)\n",
    "x = torch.randn(10, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-blake",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
